{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "K9VRtnCRlypV"
   },
   "outputs": [],
   "source": [
    "#Importing Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "xcgUVREZ017j"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/jella/vaishu mini project/fake_job_postings.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Reading Dataset\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Dataset is from https://www.kaggle.com/amruthjithrajvr/recruitment-scam\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m data\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:/Users/jella/vaishu mini project/fake_job_postings.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/jella/vaishu mini project/fake_job_postings.csv'"
     ]
    }
   ],
   "source": [
    "# Reading Dataset\n",
    "# Dataset is from https://www.kaggle.com/amruthjithrajvr/recruitment-scam\n",
    "data=pd.read_csv(\"C:/Users/jella/vaishu mini project/fake_job_postings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 600
    },
    "id": "tcscBjDC1mTb",
    "outputId": "ad818c03-da2d-443f-ecd2-15dc8657ff2a"
   },
   "outputs": [],
   "source": [
    "# Reading top 5 rows of our dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QE8F7Cmg1wcd",
    "outputId": "28836f98-a507-49f1-d4ed-7ff2704cbaf2"
   },
   "outputs": [],
   "source": [
    "# To check the number of rows and column\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ZMVWC5G138w",
    "outputId": "00486884-7da4-44f1-b122-eb6c98b73550"
   },
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qW7Gzq_I2Uw4",
    "outputId": "b12ce799-0e6a-474d-d1e8-6ca6fb942162"
   },
   "outputs": [],
   "source": [
    "# let us check the missing values in our dataset\n",
    "\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1chuj3Y12tiQ"
   },
   "outputs": [],
   "source": [
    "# Let us remove the columns which are not necessary\n",
    "# axis =1 specifies that the values are column value and inplace=true to make these changes permanent (ie. make these dropes of columns permanent in the data set)\n",
    "# We have droped salary range because 70% approx null value\n",
    "# also job_id and other irrelvent columns because they does not have any logical meaning\n",
    "data.drop(['job_id', 'salary_range', 'telecommuting', 'has_company_logo', 'has_questions'],axis=1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510
    },
    "id": "TyDIdE9p37oa",
    "outputId": "61b87267-31e1-4c98-9196-fe1525289976"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B67f9P1G44rj"
   },
   "outputs": [],
   "source": [
    "# Fill NaN values with blank space\n",
    "# inplace=true to make this change in the dataset permanent\n",
    "data.fillna(' ', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip uninstall scikit-learn\n",
    "#!pip uninstall imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn==1.2.2\n",
    "!pip install imbalanced-learn==0.10.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Gm-fQA_QD1e7",
    "outputId": "11ba3025-d27d-499b-8fa7-391fd9c1f855"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'data' is your DataFrame\n",
    "columns = data.columns.tolist()\n",
    "\n",
    "# Filter the columns to remove the target column\n",
    "columns = [c for c in columns if c not in [\"fraudulent\"]]\n",
    "\n",
    "# Store the variable we are predicting\n",
    "target = \"fraudulent\"\n",
    "\n",
    "# Define independent and dependent variables\n",
    "X = data[columns]\n",
    "Y = data[target]\n",
    "\n",
    "# Print the shapes of X & Y\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "\n",
    "# Resampling the data to address class imbalance\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "under_sampler = RandomUnderSampler(random_state=42)\n",
    "X_res, y_res = under_sampler.fit_resample(X, Y)\n",
    "\n",
    "# Creating DataFrames from the resampled data\n",
    "df1 = pd.DataFrame(X_res, columns=columns)\n",
    "df3 = pd.DataFrame(y_res, columns=[target])\n",
    "\n",
    "# Combining the resampled features and target into one DataFrame\n",
    "result = pd.concat([df1, df3], axis=1)\n",
    "\n",
    "# Displaying the result\n",
    "display(result)\n",
    "\n",
    "# Updating the original data with the resampled data\n",
    "data = result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Class distribution before resampling:\\n\", Y.value_counts())\n",
    "print(\"Class distribution after resampling:\\n\", y_res.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LfEDWoYo5Jc6",
    "outputId": "1f9a07d6-f17f-400d-8e1f-c22f14868200"
   },
   "outputs": [],
   "source": [
    "data.isnull().sum()\n",
    "# data cleaning done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98zY2kuitori"
   },
   "source": [
    "# Explaratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "id": "HvbM0mTetbBu",
    "outputId": "4af30512-ff0a-458e-f821-8c74eaca21e8"
   },
   "outputs": [],
   "source": [
    " # Checking for distribution of class label(percentages belonging to real class and percentages belonging to fraud class)\n",
    " # in the data 1 indicates fraud post\n",
    " # 0 indicating real post\n",
    " # Plotting pie chart for the data\n",
    " # function of Explode function: how the portion will appear (to understand change explode=(0,0.5))\n",
    "\n",
    "labels = 'Fake', 'Real'\n",
    "sizes = [data.fraudulent[data['fraudulent']== 1].count(), data.fraudulent[data['fraudulent']== 0].count()]\n",
    "explode = (0, 0.1) \n",
    "fig1, ax1 = plt.subplots(figsize=(8, 6)) #size of the pie chart\n",
    "ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.2f%%',\n",
    "        shadow=True, startangle=120) #autopct %1.2f%% for 2 digit precision\n",
    "ax1.axis('equal')\n",
    "plt.title(\"Proportion of Fraudulent\", size = 7)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OHoJuI6WvydE",
    "outputId": "28977f74-1326-47af-a578-3aa5f0ef4b4c"
   },
   "outputs": [],
   "source": [
    "# we will try to see which country is posting most of the jobs\n",
    "# Visualize job postings by countries\n",
    "# we will use the location column for visualizing this data\n",
    "# In location data is of type (country_name,state,city)\n",
    "# we neeed to know the country wise data\n",
    "\n",
    "def split(location):\n",
    "    l = location.split(',')\n",
    "    return l[0]\n",
    "\n",
    "data['country'] = data.location.apply(split)\n",
    "data['country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8HjC1dZk5K3o",
    "outputId": "31c48dec-3e9a-4079-ab45-6edadb9b0648"
   },
   "outputs": [],
   "source": [
    "# this will give unique country values\n",
    "data['country'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lu7L8jH36R_G",
    "outputId": "c8f7a754-953d-44af-ef77-db40eecedcc5"
   },
   "outputs": [],
   "source": [
    "# top 10 country that post jobs \n",
    "data['country'].value_counts()[:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 590
    },
    "id": "feTkqv9c156Y",
    "outputId": "19351694-9fd7-48f2-e8f9-ef1b2a3d7fad"
   },
   "outputs": [],
   "source": [
    "# creating a dictionary(key-value pair) with top 10 country\n",
    "country = dict(data.country.value_counts()[:11])\n",
    "del country[' '] #deleting country with space values\n",
    "plt.figure(figsize=(12,9))\n",
    "plt.title('Country-wise Job Posting', size=20)\n",
    "plt.bar(country.keys(), country.values()) #(xaxis,yaxis)\n",
    "plt.ylabel('No. of jobs', size=10)\n",
    "plt.xlabel('Countries', size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wSSnINUW7p2X",
    "outputId": "8c0a7c89-5939-4bfa-c5ac-0f4c631230b5"
   },
   "outputs": [],
   "source": [
    "country.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 611
    },
    "id": "a2uUftuw8E7j",
    "outputId": "e9ae460e-0487-4865-86f3-845b411e8488"
   },
   "outputs": [],
   "source": [
    "# visualizing jobs based on experience\n",
    "experience = dict(data.required_experience.value_counts())\n",
    "del experience[' ']\n",
    "plt.figure(figsize=(12,9))\n",
    "plt.bar(experience.keys(), experience.values())\n",
    "plt.title('No. of Jobs with Experience')\n",
    "plt.xlabel('Experience', size=10)\n",
    "plt.ylabel('No. of jobs', size=10)\n",
    "plt.xticks(rotation=35)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ELr9DOKfI0j7"
   },
   "outputs": [],
   "source": [
    "# Task: This data is Inbalanced, it contains 95% of real jobs and only 5% fake jobs,but we can make it balance\n",
    "# Try this out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wytm2wMeJQQd",
    "outputId": "b8d03a6e-6873-4f42-c12f-f36f929ebc83"
   },
   "outputs": [],
   "source": [
    "#Most frequent jobs\n",
    "print(data.title.value_counts()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "24z61tTSJt9Z",
    "outputId": "d4b4238a-c6f5-4904-a1ad-704050e2416f"
   },
   "outputs": [],
   "source": [
    "#Titles and count of fraudulent jobs\n",
    "# checking for most fake jobs based on title\n",
    "print(data[data.fraudulent==1].title.value_counts()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LV8eIPvMKMUU"
   },
   "outputs": [],
   "source": [
    "# For textual type data we will try to create word cloud \n",
    "# but before that we will try to create text combining all the data present in\n",
    "# our database.\n",
    "data['text'] = data['title']+' '+data['location']+' '+data['company_profile']+' '+data['description']+' '+data['requirements']+' '+data['benefits']+' '+data['industry']\n",
    "\n",
    "del data['title']\n",
    "del data['location']\n",
    "del data['department']\n",
    "del data['company_profile']\n",
    "del data['description']\n",
    "del data['requirements']\n",
    "del data['benefits']\n",
    "del data['required_experience']\n",
    "del data['required_education']\n",
    "del data['industry']\n",
    "del data['function']\n",
    "del data['country']\n",
    "del data['employment_type']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "hb7AUje2MDVj",
    "outputId": "36df0b07-b1ba-4dc3-9ea3-f33cbf162e67"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_lz9fC5MqaX"
   },
   "source": [
    "**Understanding the common words used in the texts : Wordcloud**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EewDWuZROwOS"
   },
   "outputs": [],
   "source": [
    "# we will plot 3 kind of word cloud\n",
    "# 1st we will visualize all the words our data using the wordcloud plot\n",
    "# 2nd we will visualize common words in real job posting\n",
    "# 3rd we will visualize common words in fraud job posting\n",
    "# join function is a core python function\n",
    "from wordcloud import WordCloud\n",
    "all_words = ''.join([text for text in data[\"text\"]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s_HQntGKPigf"
   },
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width = 800, height = 500, random_state=21, max_font_size=120).generate(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "G7P_s2g-P_qd",
    "outputId": "07992aa8-d628-4b3f-d620-06ef29e7c358"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bY3tMBj0zo4-"
   },
   "outputs": [],
   "source": [
    " # Common words in real job posting texts\n",
    "\n",
    "real_post = ''.join([text for text in data[\"text\"][data['fraudulent']==0]])\n",
    "wordcloud = WordCloud(width = 800, height = 500, random_state=21, max_font_size=120).generate(real_post)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "Dy_gjyoq5Kfl",
    "outputId": "08ee719c-3589-453a-bfdb-14f61d42b886"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hXYwhwiE6a_u"
   },
   "outputs": [],
   "source": [
    "# Common words in fraud job posting texts\n",
    "\n",
    "fraud_post = ''.join([text for text in data[\"text\"][data['fraudulent'] == 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1YpfofCK6ssc"
   },
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width = 800, height = 500, random_state=21, max_font_size=120).generate(fraud_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "eATGwSOi6w_v",
    "outputId": "ef52ce86-f5a0-4b20-e12f-a1aa40577df7"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AnHq3gUU7YJt"
   },
   "source": [
    "## Data *Preapration*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nneU3dAA7b3I",
    "outputId": "861c6a47-3ee0-4f94-c000-c2293b8927ef"
   },
   "outputs": [],
   "source": [
    "# NLTK :: Natural Language Toolkit\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "06YsOVFR8Hkf",
    "outputId": "1ca6195e-5b87-4b99-b446-25a145779211"
   },
   "outputs": [],
   "source": [
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H7ExCCQi8T9o"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Text Preprocessing Function\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove special characters\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    tokens = text.split()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stopwords.words('english')]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to the text data\n",
    "data['text'] = data['text'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "id": "LM3ovfV087zk",
    "outputId": "88a909d5-7353-4d6f-9aed-7480c18bccfb"
   },
   "outputs": [],
   "source": [
    "data['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ED2SndnB-c0"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Splitting dataset in train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.text, data.fraudulent, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "brl98cDfDIjy",
    "outputId": "ed5f3c9a-e74d-4b8d-9282-6cbfbc1671bb"
   },
   "outputs": [],
   "source": [
    "# what does X-train and y_train contain\n",
    "print(y_train)\n",
    "print(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xkW_jatPOi1E"
   },
   "outputs": [],
   "source": [
    "# The model cannot operate text data so we need to convert our data into vector format\n",
    "# we will be using Bag of words model \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#  instantiate the vectorizer\n",
    "vect = CountVectorizer()\n",
    "\n",
    "# learn training data vocabulary, then use it to create a document-term matrix\n",
    "# fit\n",
    "vect.fit(X_train)\n",
    "\n",
    "# transform training data\n",
    "X_train_dtm = vect.transform(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-kx_rM9hR98a",
    "outputId": "3f07a425-a540-4cba-e10b-d496d303584b"
   },
   "outputs": [],
   "source": [
    "# examine the document-term matrix\n",
    "X_train_dtm\n",
    "\n",
    "#how X_train_dtm is looking\n",
    "print(X_train_dtm)\n",
    "# This is Matrix representation,non 0 valued cells are not printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UKBzzVOYz7Qw"
   },
   "outputs": [],
   "source": [
    "# PCA is not advisable for a NLP task,because PCA is for \n",
    "# principal feature extraction and in this case we already have features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QOy4vFrCMcYS",
    "outputId": "4ccb8f8f-1a18-4b4e-d664-74e773be6d0c"
   },
   "outputs": [],
   "source": [
    "# transform testing data (using fitted vocabulary) into a document-term matrix\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "X_test_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SRvNcfBM-K6H"
   },
   "source": [
    "# Model Building & evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jHweBsPO-TUB"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytjbwXWBQjVg"
   },
   "source": [
    "<h2>Naive Bayes</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I-zWilrWNUo-",
    "outputId": "36dee0cc-5e81-4280-bf73-9c6c1007dd0e"
   },
   "outputs": [],
   "source": [
    "# Naive Bayes Classifier\n",
    "# we are using Multinomial Naive Bayes approach because the data here is not symmetrical.\n",
    "# generally if there are data in the form of this long text,it is advisable to \n",
    "# %time will give the time taken by the system for execution\n",
    "nb = MultinomialNB()\n",
    "%time nb.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6IxQ99VrQUJh",
    "outputId": "b597aa08-6282-4192-a2c1-669cbab13f90"
   },
   "outputs": [],
   "source": [
    "y_pred_nb = nb.predict(X_test_dtm)\n",
    "print(y_pred_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 752
    },
    "id": "cJtxO-BuQZJv",
    "outputId": "18d2b703-e5af-4103-fa74-c968e8b156e9"
   },
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "\n",
    "accuracy_score(y_test, y_pred_nb)\n",
    "print(\"Classification Accuracy:\", accuracy_score(y_test, y_pred_nb))\n",
    "print(\"Classification Report\\n\")\n",
    "print(classification_report(y_test, y_pred_nb))\n",
    "print(\"Confusion Matrix\\n\")\n",
    "print(confusion_matrix(y_test, y_pred_nb))\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_test,y_pred_nb)\n",
    "\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(cm, annot=True, fmt='d')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC-AUC Score and Plot for Naive Bayes\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_prob_nb = nb.predict_proba(X_test_dtm)[:, 1]\n",
    "\n",
    "fpr_nb, tpr_nb, _ = roc_curve(y_test, y_prob_nb)\n",
    "roc_auc_nb = roc_auc_score(y_test, y_prob_nb)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(fpr_nb, tpr_nb, label=f'Naive Bayes (AUC = {roc_auc_nb:.2f})')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-zx-DlNRRXA2"
   },
   "source": [
    "<h2>Decision Tree Classifier</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dqRzBnpFRemL"
   },
   "outputs": [],
   "source": [
    "#instantiate a Decision Tree Classifier\n",
    "dt = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PioI8RY9RvSy",
    "outputId": "63f2fe49-1784-4697-aa5a-d095fc4eb7ac"
   },
   "outputs": [],
   "source": [
    "#train the model \n",
    "# using X_train_dtm (timing it with an IPython \"magic command\")\n",
    "\n",
    "%time dt.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M2kLebUXSH_5"
   },
   "outputs": [],
   "source": [
    "# make class predictions for X_test_dtm\n",
    "y_pred_class = dt.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WBBlK4TiSZWb",
    "outputId": "2ff6dae5-7d98-4cf3-b223-1a636eba9df2"
   },
   "outputs": [],
   "source": [
    "# Model Accuracy\n",
    "print(\"Classification Accuracy:\", accuracy_score(y_test, y_pred_class))\n",
    "print(\"Classification Report\\n\")\n",
    "print(classification_report(y_test, y_pred_class))\n",
    "print(\"Confusion Matrix\\n\")\n",
    "print(confusion_matrix(y_test, y_pred_class))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 460
    },
    "id": "Vo11xhvSSZca",
    "outputId": "52c2e104-9403-4a19-a809-f4bf0f20301b"
   },
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "\n",
    "import seaborn as sn\n",
    "cm = confusion_matrix(y_test,y_pred_class)\n",
    "\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(cm, annot=True, fmt='d')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC-AUC Score and Plot for Decision tree classifier\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_prob_dt = dt.predict_proba(X_test_dtm)[:, 1]\n",
    "\n",
    "fpr_dt, tpr_dt, _ = roc_curve(y_test, y_prob_dt)\n",
    "roc_auc_dt = roc_auc_score(y_test, y_prob_dt)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(fpr_dt, tpr_dt, label=f'Decision tree classifier (AUC = {roc_auc_dt:.2f})')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "lr_model.fit(X_train_dtm, y_train)\n",
    "y_pred_lr = lr_model.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
    "print(\"Logistic Regression Classification Report\\n\")\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "print(cm_lr)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')\n",
    "plt.title('Logistic Regression Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC-AUC Score and Plot for Logistic regresson\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_prob_lr = lr_model.predict_proba(X_test_dtm)[:, 1]\n",
    "\n",
    "fpr_lr, tpr_lr, _ = roc_curve(y_test, y_prob_lr)\n",
    "roc_auc_lr = roc_auc_score(y_test, y_prob_lr)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(fpr_lr, tpr_lr, label=f'Logistic Regression (AUC = {roc_auc_lr:.2f})')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators=100)\n",
    "rf_model.fit(X_train_dtm, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"Random Forest Classification Report\\n\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "print(cm_rf)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')\n",
    "plt.title('Random Forest Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC-AUC Score and Plot for Random Forest\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_prob_rf = rf_model.predict_proba(X_test_dtm)[:, 1]\n",
    "\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_prob_rf)\n",
    "roc_auc_rf = roc_auc_score(y_test, y_prob_rf)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC = {roc_auc_rf:.2f})')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = SVC(probability=True)\n",
    "svm_model.fit(X_train_dtm, y_train)\n",
    "y_pred_svm = svm_model.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
    "print(\"SVM Classification Report\\n\")\n",
    "print(classification_report(y_test, y_pred_svm))\n",
    "cm_svm = confusion_matrix(y_test, y_pred_svm)\n",
    "print(cm_svm)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm_svm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')\n",
    "plt.title('SVM Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC-AUC Score and Plot for SVM\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_prob_svm = svm_model.predict_proba(X_test_dtm)[:, 1]\n",
    "\n",
    "fpr_svm, tpr_svm, _ = roc_curve(y_test, y_prob_svm)\n",
    "roc_auc_svm = roc_auc_score(y_test, y_prob_svm)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(fpr_svm, tpr_svm, label=f'SVM (AUC = {roc_auc_svm:.2f})')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib  # To save and load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model and vectorizer\n",
    "joblib.dump(rf_model, 'random_forest_model.pkl')\n",
    "joblib.dump(vect, 'bow_vectorizer.pkl')\n",
    "\n",
    "# Load the model and make predictions on new data\n",
    "def predict_fraudulent_job_posting(text):\n",
    "    # Load the model and vectorizer\n",
    "    rf_model = joblib.load('random_forest_model.pkl')\n",
    "    vect = joblib.load('bow_vectorizer.pkl')\n",
    "    \n",
    "    # Preprocess the input text\n",
    "    text_dtm = vect.transform([text])\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = rf_model.predict(text_dtm)\n",
    "    \n",
    "    return prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "new_text = X_test[547]\n",
    "prediction = predict_fraudulent_job_posting(new_text)\n",
    "print(\"Prediction (0 = Real, 1 = Fraudulent):\", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test[547])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Final Fake_job_predictor.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
